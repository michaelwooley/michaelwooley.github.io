from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium import webdriver

class ScrapeDynamic(object):
    """
    ScrapeDynamic: Methods for scraping dynamic webpages.

    Information on:
        Basic concept: https://coderwall.com/p/vivfza/fetch-dynamic-web-pages-with-selenium
        Selenium Scrolling: https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python
        Selenium waiting: http://selenium-python.readthedocs.io/waits.html

    Be sure to call ScrapeDynamic.stop() when you're done to shut down the
        server thing
    """

    def __init__(self, browserPath, browser='phantom'):
        """
        Input:
            - browserPath: Path to
            - browser: Browser to use ['phantom', 'firefox']. (default='phantom')
        Returns:
            A ScrapeDynamic object.
        """
        # Start the WebDriver and load the page
        self.wd = webdriver.PhantomJS(executable_path = BrowserPath)

    def getUrl(self, url, selector):
        """
        Retrieve page source of dynamic webpage. Waits until `selector` loads to
            return. Automatically scrolls to bottom of page to ensure that all
            JS loads.

        Inputs:
            - url: website url
            - selector: CSS selector

        Returns:
            Page source (i.e. suitable for BeautifulSoup).
        """
        # Begin to retrieve the URL
        self.wd.get(url)
        # Scroll to bottom to page so that it will load all elements
        self.wd.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # Wait for the dynamically loaded elements to show up
        WebDriverWait(self.wd, 10).until(
                EC.visibility_of_element_located((By.CSS_SELECTOR, selector)))

        return self.wd.page_source

    def stop(self):
        self.wd.quit()


if __name__ == "__main__":
    from bs4 import BeautifulSoup

    BrowserPath = 'C:/Program Files/PhantomJS/bin/phantomjs.exe' # Path to browser .exe
    URL = 'http://www.uslsoccer.com/newyorkredbullsii-fccincinnati-905766' # URL to retrieve
    selector = 'table.Opta-Striped.Opta-Squad' # CSS element to wait for

    R = ScrapeDynamic(BrowserPath)
    html_page = R.getUrl(URL, selector)
    R.stop()

    soup = BeautifulSoup(html_page, 'lxml')
    element = soup.select(selector)
    print(element[0].prettify()[0:1000])